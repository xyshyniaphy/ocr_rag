#!/usr/bin/env python3
"""
Text Chunking Service Test Script
Tests the chunking service with sample OCR results
"""

import asyncio
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from backend.core.logging import get_logger
from backend.services.ocr import ocr_pdf, OCROptions
from backend.services.processing import ChunkingService, ChunkingOptions, chunk_document

logger = get_logger(__name__)


async def test_basic_chunking():
    """Test basic text chunking"""

    print("=" * 60)
    print("Text Chunking Service Test")
    print("=" * 60)

    # Sample Japanese text
    sample_text = """
    ç¬¬1ç«  ç·å‰‡

    ç¬¬1æ¡ï¼ˆç›®çš„ï¼‰
    ã“ã®æ³•å¾‹ã¯ã€é›»ç£çš„æ–¹æ³•ã«ã‚ˆã‚‹å¥‘ç´„ã®ç· çµç­‰ã«é–¢ã™ã‚‹è¦å¾‹ã‚’æ•´å‚™ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€
    é›»å­å•†å–å¼•ã®å††æ»‘åŒ–ã‚’å›³ã‚Šã€ã‚‚ã£ã¦å›½æ°‘çµŒæ¸ˆã®å¥å…¨ãªç™ºå±•ã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã™ã‚‹ã€‚

    ç¬¬2æ¡ï¼ˆå®šç¾©ï¼‰
    ã“ã®æ³•å¾‹ã«ãŠã„ã¦ã€Œé›»ç£çš„æ–¹æ³•ã€ã¨ã¯ã€é›»å­æƒ…å ±å‡¦ç†çµ„ç¹”ã‚’ä½¿ç”¨ã™ã‚‹æ–¹æ³•ãã®ä»–ã®
    æƒ…å ±é€šä¿¡ã®æŠ€è¡“ã‚’åˆ©ç”¨ã™ã‚‹æ–¹æ³•ã§ã‚ã£ã¦ã€æ¬¡ã«æ²ã’ã‚‹ã‚‚ã®ã‚’ã„ã†ã€‚

    ä¸€ é›»å­è¨ˆç®—æ©Ÿãã®ä»–ã®æƒ…å ±å‡¦ç†æ©Ÿå™¨ã«ä¿‚ã‚‹å…¥åŠ›è£…ç½®ã‹ã‚‰ã€å½“è©²æƒ…å ±å‡¦ç†æ©Ÿå™¨ã«
    ä¿‚ã‚‹å‡ºåŠ›è£…ç½®ã¸æƒ…å ±ã‚’é€ä¿¡ã™ã‚‹æ–¹æ³•

    äºŒ å‰å·ã«æ²ã’ã‚‹æ–¹æ³•ã«æº–ãšã‚‹æ–¹æ³•ã¨ã—ã¦ã€ä¸»å‹™çœä»¤ã§å®šã‚ã‚‹æ–¹æ³•

    ç¬¬3æ¡ï¼ˆé©ç”¨ç¯„å›²ï¼‰
    ã“ã®æ³•å¾‹ã¯ã€äº‹æ¥­è€…é–“ã®å–å¼•åŠã³äº‹æ¥­è€…ã¨æ¶ˆè²»è€…ã¨ã®é–“ã®å–å¼•ã«ã¤ã„ã¦é©ç”¨ã™ã‚‹ã€‚
    ãŸã ã—ã€æ¶ˆè²»è€…å¥‘ç´„æ³•ï¼ˆå¹³æˆåäºŒå¹´æ³•å¾‹ç¬¬å…­åä¸€å·ï¼‰ç¬¬äºŒç« ç¬¬å››ç¯€ã®è¦å®šã®
    é©ç”¨ã‚’å—ã‘ã‚‹å–å¼•ã«ã¤ã„ã¦ã¯ã€ã“ã®é™ã‚Šã§ãªã„ã€‚
    """

    print("\n" + "-" * 40)
    print("Test 1: Basic Text Chunking")
    print("-" * 40)

    # Test with default options
    options = ChunkingOptions(
        chunk_size=200,
        chunk_overlap=30,
    )

    service = ChunkingService(options=options, strategy="recursive")

    chunks = await service.chunk_text(
        text=sample_text,
        document_id="test-doc-001",
    )

    print(f"\nGenerated {len(chunks)} chunks:")
    for i, chunk in enumerate(chunks, 1):
        print(f"\nChunk {i}:")
        print(f"  Size: {len(chunk.text)} chars")
        print(f"  Tokens: {chunk.token_count}")
        print(f"  Type: {chunk.metadata.chunk_type}")
        print(f"  Preview: {chunk.text[:100]}...")


async def test_ocr_chunking():
    """Test chunking with real OCR results"""

    print("\n" + "=" * 60)
    print("OCR + Chunking Integration Test")
    print("=" * 60)

    # Check if test PDF exists
    test_pdf_path = Path("/app/backend/testdata/test.pdf")
    if not test_pdf_path.exists():
        test_pdf_path = Path("testdata/test.pdf")

    if not test_pdf_path.exists():
        print(f"ERROR: Test PDF not found at {test_pdf_path}")
        print("Skipping OCR chunking test")
        return

    print(f"\nTest PDF: {test_pdf_path}")

    # Read and OCR the PDF
    with open(test_pdf_path, "rb") as f:
        pdf_bytes = f.read()

    print("\nRunning OCR...")
    ocr_result = await ocr_pdf(pdf_bytes)

    print(f"\nOCR Result:")
    print(f"  Engine: {ocr_result.engine_used}")
    print(f"  Pages: {ocr_result.total_pages}")
    print(f"  Confidence: {ocr_result.confidence:.2%}")

    # Test different chunking strategies
    strategies = ["recursive", "semantic", "table_aware"]

    for strategy in strategies:
        print("\n" + "-" * 40)
        print(f"Strategy: {strategy}")
        print("-" * 40)

        try:
            options = ChunkingOptions(
                chunk_size=300,
                chunk_overlap=40,
            )

            chunking_result = await chunk_document(
                ocr_result=ocr_result,
                document_id="test-doc-002",
                chunk_size=300,
                chunk_overlap=40,
                strategy=strategy,
            )

            print(f"\nChunks: {chunking_result.total_chunks}")
            print(f"Total chars: {chunking_result.total_characters}")
            print(f"Total tokens: {chunking_result.total_tokens}")
            print(f"Avg chunk size: {chunking_result.avg_chunk_size:.1f} chars")
            print(f"Processing time: {chunking_result.processing_time_ms}ms")

            # Show first few chunks
            print(f"\nFirst 3 chunks:")
            for i, chunk in enumerate(chunking_result.chunks[:3], 1):
                print(f"\n  Chunk {i} (Page {chunk.metadata.page_number}):")
                print(f"    Size: {len(chunk.text)} chars, {chunk.token_count} tokens")
                print(f"    Type: {chunk.metadata.chunk_type}")
                print(f"    Preview: {chunk.text[:80]}...")

            if chunking_result.warnings:
                print(f"\nWarnings: {len(chunking_result.warnings)}")
                for warning in chunking_result.warnings[:3]:
                    print(f"  - {warning}")

        except Exception as e:
            print(f"ERROR with {strategy} strategy: {e}")
            import traceback
            traceback.print_exc()


async def test_japanese_awareness():
    """Test Japanese-specific chunking features"""

    print("\n" + "=" * 60)
    print("Japanese-Aware Chunking Test")
    print("=" * 60)

    # Japanese text with various sentence structures
    text = """
    æ—¥æœ¬èªã®æ–‡ç« ã¯æ¼¢å­—ã€ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€ãã—ã¦ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ã€‚
    æ–‡ã®çµ‚ã‚ã‚Šã«ã¯å¥èª­ç‚¹ãŒä½¿ã‚ã‚Œã¾ã™ã€‚æ—¥æœ¬èªã®æ–‡ç« ã§ã¯ã€Œã€‚ã€ãŒæœ€ã‚‚ä¸€èˆ¬çš„ãªå¥ç‚¹ã§ã™ï¼
    ç–‘å•æ–‡ã«ã¯ã€Œï¼Ÿã€ã‚’ä½¿ã„ã¾ã™ãŒã€ã“ã‚Œã¯æ¯”è¼ƒçš„æ–°ã—ã„å‚¾å‘ã§ã™ã€‚

    ç¬¬1ç« ã€€æ¦‚è¦
    ã“ã®ç« ã§ã¯ã‚·ã‚¹ãƒ†ãƒ ã®æ¦‚è¦ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ ã¯ä¸»ã«3ã¤ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰
    æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚ãã‚Œã‚‰ã¯ã€OCRã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã€ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã€ãã—ã¦
    æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã™ã€‚

    ç¬¬2ç« ã€€è©³ç´°è¨­è¨ˆ
    è©³ç´°è¨­è¨ˆã§ã¯ã€å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ä»•æ§˜ã‚’å®šç¾©ã—ã¾ã™ã€‚ã¾ãšOCRã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰
    å§‹ã‚ã¾ã™ã€‚OCRã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯YomiTokuã¨PaddleOCRã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
    """

    print("\n" + "-" * 40)
    print("Japanese Sentence Boundary Detection")
    print("-" * 40)

    options = ChunkingOptions(
        chunk_size=150,
        chunk_overlap=20,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ã€"],
    )

    service = ChunkingService(options=options, strategy="recursive")
    chunks = await service.chunk_text(text, "test-doc-003")

    print(f"\nGenerated {len(chunks)} chunks:")
    for i, chunk in enumerate(chunks, 1):
        print(f"\nChunk {i}:")
        print(f"  Text: {chunk.text}")
        print(f"  Ends with sentence marker: {chunk.text[-1] in 'ã€‚ï¼ï¼Ÿ'}")


async def main():
    """Main test runner"""

    print("\n")
    print("â•”" + "â•" * 58 + "â•—")
    print("â•‘" + " " * 10 + "Text Chunking Test Suite" + " " * 27 + "â•‘")
    print("â•š" + "â•" * 58 + "â•")

    success = True

    # Test 1: Basic chunking
    try:
        await test_basic_chunking()
    except Exception as e:
        print(f"\nERROR in basic chunking test: {e}")
        import traceback
        traceback.print_exc()
        success = False

    # Test 2: OCR + Chunking integration
    try:
        await test_ocr_chunking()
    except Exception as e:
        print(f"\nERROR in OCR chunking test: {e}")
        import traceback
        traceback.print_exc()
        success = False

    # Test 3: Japanese awareness
    try:
        await test_japanese_awareness()
    except Exception as e:
        print(f"\nERROR in Japanese awareness test: {e}")
        import traceback
        traceback.print_exc()
        success = False

    if success:
        print("\n" + "ğŸ‰" + " All tests passed! " + "ğŸ‰")
        sys.exit(0)
    else:
        print("\n" + "âŒ Some tests failed")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
