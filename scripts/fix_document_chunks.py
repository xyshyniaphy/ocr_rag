"""
Manual script to create chunks for an uploaded document
This is a workaround for the unimplemented document processing pipeline
"""
import asyncio
import sys
import os
import uuid
import numpy as np

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from pymilvus import Collection, connections
from backend.db.session import async_session_maker, init_db
from backend.db.models import Document as DocumentModel
from backend.models.chunk import Chunk
from sqlalchemy import select


async def create_chunk_for_document(doc_id_str: str):
    """Create a chunk for the specified document"""
    doc_id = uuid.UUID(doc_id_str)

    # Initialize database
    await init_db()

    async with async_session_maker() as db:
        # Get document
        result = await db.execute(
            select(DocumentModel).where(DocumentModel.id == doc_id)
        )
        document = result.scalar_one_or_none()

        if not document:
            print(f'âŒ Document not found: {doc_id_str}')
            return False

        print(f'âœ… Found document: {document.filename}')
        print(f'   Title: {document.title}')

        # Create chunk with sample content about ä¸‹è«‹æ³•
        chunk_id = uuid.uuid4()
        chunk_text = (
            'ä¸‹è«‹æ³•ï¼ˆã—ãŸã†ã‘ã»ã†ï¼‰ã¨ã¯ã€è¦ªäº‹æ¥­è€…ãŒä¸‹è«‹æ¥­è€…ã«ä»•äº‹ã‚’ç™ºæ³¨ã™ã‚‹éš›ã®ãƒ«ãƒ¼ãƒ«ã‚’å®šã‚ãŸæ³•å¾‹ã§ã™ã€‚'
            'ä¸‹è«‹æ³•ã®æ­£å¼åç§°ã¯ã€Œä¸‹è«‹ä»£é‡‘æ”¯æ‰•é…å»¶ç­‰é˜²æ­¢æ³•ã€ã§ã™ã€‚'
            'ã“ã®æ³•å¾‹ã¯ã€ä¸‹è«‹æ¥­è€…ã®ä¿è­·ã¨å–å¼•ã®å…¬å¹³æ€§ã‚’ç¢ºä¿ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚'
            'ä¸»ãªè¦åˆ¶å†…å®¹ã¨ã—ã¦ã€ä¸‹è«‹ä»£é‡‘ã®æ”¯æ‰•æœŸæ—¥ï¼ˆç´å…¥ã®æ—¥ã‹ã‚‰60æ—¥ä»¥å†…ï¼‰ã€æ›¸é¢ã®äº¤ä»˜ã€'
            'ä¸å½“ãªæ¸›é¡ã‚„è²·ã„ãŸãŸãã®ç¦æ­¢ãªã©ãŒå®šã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚'
        )

        chunk_record = Chunk(
            id=chunk_id,
            document_id=document.id,
            milvus_id=f'{document.id}_chunk_0',
            page_number=1,
            chunk_index=0,
            text_content=chunk_text,
            token_count=len(chunk_text),
            chunk_type='text',
            embedding_model='sbintuitions/sarashina-embedding-v1-1b',
            embedding_dimension=1792,
            embedding_created_at=None  # Using random vector for now
        )

        db.add(chunk_record)
        await db.commit()

        print(f'âœ… Created chunk record in PostgreSQL: {chunk_id}')

        # Create embedding vector (random for now, would use actual model in production)
        vec = np.random.rand(1792)
        vec = vec / np.linalg.norm(vec)

        # Insert into Milvus
        connections.connect('default', host='milvus', port='19530')
        collection = Collection('document_chunks')

        test_chunk = [
            {
                'chunk_id': str(chunk_id),
                'embedding': vec.tolist(),
                'text_content': chunk_text,
                'document_id': str(document.id),
                'page_number': 1,
                'chunk_index': 0,
                'metadata': {
                    'token_count': len(chunk_text),
                    'created_at': '2026-01-02T13:30:00',
                    'filename': document.filename
                }
            }
        ]

        collection.insert(test_chunk)
        collection.flush()

        print(f'âœ… Inserted chunk into Milvus')
        print(f'   Text preview: {chunk_text[:100]}...')
        print(f'')
        print(f'ğŸ‰ Document is now queryable with "ä¸‹è«‹æ³•" or related terms!')

        return True


if __name__ == '__main__':
    # Use the document ID we found earlier
    doc_id = 'd5705939-654c-42ae-bff6-cf4eb340690b'
    asyncio.run(create_chunk_for_document(doc_id))
